{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feedForwardNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNX6ne1HrfhP",
        "colab_type": "text"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yuZljmdrH4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle \n",
        "from sklearn.decomposition import PCA\n",
        "import sys\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z5kzbQUx4NH",
        "colab_type": "text"
      },
      "source": [
        "# Creating the skeleton of a network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvwhW_2OwgOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createnetwork(num_hidden, activation_func, sizes, inputsize = 784, outputsize = 10):\n",
        "    # Appending the first element(number of inputs=784) and the last element(number of of outputs=10) to sizes array.\n",
        "    # Sizes is a list representing the number of units in each layer.\n",
        "    sizes = [inputsize] + sizes\n",
        "    sizes = sizes + [outputsize]\n",
        "    # Ex output of the above 2 lines: [784, 50, 100, 150, 10]. If there are 3 hidden layers with 50, 100 and 150 neurons in each layer respectively.\n",
        "    np.random.seed(1234)\n",
        "    # creating a dictionary for all the weights and biases\n",
        "    parameters = {}\n",
        "    if activation_func == \"relu\":\n",
        "        for i in range(1, num_hidden+2):\n",
        "          # Kaiming initialization for W, b i set to zeros\n",
        "            parameters[\"W\" + str(i)] = 0.01*np.random.randn(sizes[i], sizes[i-1])*(np.sqrt(2/(sizes[i] + sizes[i-1])))\n",
        "            parameters[\"b\" + str(i)] = np.zeros((sizes[i],1))\n",
        "    else:\n",
        "        for i in range(1, num_hidden+2):\n",
        "            parameters[\"W\" + str(i)] = np.random.randn(sizes[i], sizes[i-1])\n",
        "            parameters[\"b\" + str(i)] = np.random.randn(sizes[i],1)\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6Nf-J-t5Ybm",
        "colab_type": "text"
      },
      "source": [
        "# Some mathematical functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WamSYVIi4y_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "\treturn 1/(1 + np.exp(-z))\n",
        "\n",
        "def tanh(z):\n",
        "\treturn np.tanh(z)\n",
        "\n",
        "def softmax(z):\n",
        "    z = z-np.max(z)\n",
        "    numer = np.exp(z)\n",
        "    denom = np.sum(numer, axis = 0)\n",
        "    return numer/denom\n",
        "\n",
        "def convert_to_onehot(indices, num_classes):\n",
        "    output = np.eye(num_classes)[np.array(indices).reshape(-1)]\n",
        "    # each target vector is converted to a row vector. Each label is now a 10 dimensional vector.\n",
        "    return output.reshape(list(np.shape(indices))+[num_classes])\n",
        "\n",
        "def squared_loss(X, Y):\n",
        "    x = np.array(X)\n",
        "    y = np.array(Y)\n",
        "    # to hold off broadcasting behaviour in case of 1D arrays\n",
        "    if x.ndim == 1:\n",
        "        x = x[:,np.newaxis]\n",
        "    if y.ndim == 1:\n",
        "        y = y[:, np.newaxis]\n",
        "    loss = 0.5*np.sum((y-x)**2)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def cross_entropy_loss(X, Y):\n",
        "    x = np.array(X)\n",
        "    y = np.array(Y)\n",
        "    # to hold off broadcasting behaviour in case of 1D arrays\n",
        "    if x.ndim == 1:\n",
        "        x = x[:,np.newaxis]\n",
        "    if y.ndim == 1:\n",
        "        y = y[:, np.newaxis]\n",
        "    loss_vec = -1*y*np.log(x)\n",
        "    return np.sum(loss_vec) \n",
        "    # alternate implementation\n",
        "    # x = np.array(X).reshape(-1)\n",
        "    # y = np.array(Y).reshape(-1)\n",
        "    # logx = np.log(x)\n",
        "    # loss_vec = (-1)*(y*logx)\n",
        "    # loss = np.sum(loss_vec)\n",
        "    # return loss    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUv5vFLe5enL",
        "colab_type": "text"
      },
      "source": [
        "# Function for reading data and function for activating a neuron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZMRyox95abq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(path_to_csv):\n",
        "    df = pd.read_csv(path_to_csv)\n",
        "    data = df.to_numpy()\n",
        "    X = data[:,1:-1]\n",
        "    y = data[:,-1]\n",
        "    Y = [int(i) for i in y]\n",
        "    return data, X.T, Y\n",
        "\n",
        "def activate(z, activation):\n",
        "    if activation == \"sigmoid\":\n",
        "        return sigmoid(z)\n",
        "    elif activation == \"tanh\":\n",
        "        return tanh(z)\n",
        "    elif activation == \"relu\":\n",
        "        return (z>0)*(z) + 0.01*((z<0)*z)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anb_ubDl5vOI",
        "colab_type": "text"
      },
      "source": [
        "# Forward pass function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVIREMEr5d9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_pass(X, parameters, activation, num_hidden):\n",
        "    A = {}\n",
        "    # To prevent broadcasting when a single input vector is given\n",
        "    if X.ndim == 1:\n",
        "        X = X[:, np.newaxis] \n",
        "\n",
        "    #  Initializing the pre-activation to inputs \n",
        "    H = {\"h0\":X}\n",
        "\n",
        "    for l in range(1, num_hidden + 2):\n",
        "        Wl = parameters[\"W\" + str(l)]\n",
        "        bl = parameters[\"b\" + str(l)]\n",
        "        \n",
        "        hprev = H[\"h\" + str(l-1)]\n",
        "        al = np.dot(Wl,hprev) + bl\n",
        "        A[\"a\" + str(l)] = al\n",
        "        \n",
        "        if l != num_hidden + 1: \n",
        "            hl = activate(al, activation)\n",
        "        elif l == num_hidden + 1:\n",
        "            hl = softmax(al)\n",
        "        H[\"h\" + str(l)] = hl\n",
        "\n",
        "    yhat = H[\"h\" + str(num_hidden + 1)]\n",
        "    return yhat, A, H "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CtsRw9T51wV",
        "colab_type": "text"
      },
      "source": [
        "# Functions for generating gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc0Bl0Go5xF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def creategrads(num_hidden, sizes, inputsize = 784, outputsize = 10):\n",
        "    sizes = [inputsize] + sizes\n",
        "    sizes = sizes + [outputsize]\n",
        "    grads = {\"dh0\":np.zeros((inputsize,1)),\n",
        "            \"da0\":np.zeros((inputsize,1))}\n",
        "    for i in range(1, num_hidden+2):\n",
        "        grads[\"dW\" + str(i)] = np.zeros((sizes[i], sizes[i-1]))\n",
        "        grads[\"db\" + str(i)] = np.zeros((sizes[i],1))\n",
        "        grads[\"da\" + str(i)] = np.zeros((sizes[i],1))\n",
        "        grads[\"dh\" + str(i)] = np.zeros((sizes[i],1))\n",
        "    return grads\n",
        "\n",
        "\n",
        "def grad_sigmoid(z):\n",
        "    return (sigmoid(z))*(1 - sigmoid(z))\n",
        "\n",
        "def grad_tanh(z):\n",
        "    return (1 - (np.tanh(z))**2)\n",
        "\n",
        "def grad_relu(z):\n",
        "    return (z>0)*(np.ones(np.shape(z))) + (z<0)*(0.01*np.ones(np.shape(z)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5VlAEoU59Oe",
        "colab_type": "text"
      },
      "source": [
        "# Function for back propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcBB0uMm54i8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def back_prop(H, A, parameters, num_hidden, sizes, Y, Yhat, loss, activation, inputsize, outputsize):\n",
        "    grad_one_eg = creategrads(num_hidden, sizes, inputsize = 784, outputsize = 10)\n",
        "    A[\"a0\"] = np.zeros((inputsize,1))\n",
        "    if Y.ndim == 1:\n",
        "        Y = Y[:, np.newaxis]\n",
        "    if Yhat.ndim == 1:\n",
        "        Yhat = Yhat[:, np.newaxis]\n",
        "\n",
        "    if loss == \"ce\":\n",
        "      # Derivative of loss function with respect to the pre-activations of the output layer('a').\n",
        "        grad_one_eg[\"da\" + str(num_hidden + 1)] = Yhat - Y\n",
        "    elif loss == \"sq\":\n",
        "        grad_one_eg[\"da\" + str(num_hidden + 1)] = (Yhat - Y)*Yhat - Yhat*(np.dot((Yhat-Y).T, Yhat))\n",
        "\n",
        "    for i in np.arange(num_hidden + 1, 0, -1):\n",
        "            # a = b + W*h. \n",
        "            # grad_one_eg[\"da\" + str(i)] this part of the below dot product is calculated at the end of this for loop except for the activation at the output layer.\n",
        "\n",
        "            # Derivative of loss function with respect to the weight matrix\n",
        "            grad_one_eg[\"dW\" + str(i)] = np.dot(grad_one_eg[\"da\" + str(i)], (H[\"h\" + str(i-1)]).T)\n",
        "            # Derivative of loss function with respect to the weight matrix\n",
        "            grad_one_eg[\"db\" + str(i)] = grad_one_eg[\"da\" + str(i)]\n",
        "            # Derivative of loss function wrt activation 'h'. \n",
        "            grad_one_eg[\"dh\" + str(i-1)] = np.dot((parameters[\"W\" + str(i)]).T, grad_one_eg[\"da\" + str(i)])\n",
        "\n",
        "            if activation == \"sigmoid\":\n",
        "                derv = grad_sigmoid(A[\"a\" + str(i-1)])\n",
        "            elif activation == \"tanh\":\n",
        "                derv = grad_tanh(A[\"a\" + str(i-1)])\n",
        "            elif activation == \"relu\":\n",
        "                derv = grad_relu(A[\"a\" + str(i-1)])\n",
        "            if derv.ndim == 1:\n",
        "                derv = derv[:, np.newaxis]\n",
        "            # Derivative of w.r.t to pre-activations('a') of the hidden layers. \n",
        "            grad_one_eg[\"da\" + str(i-1)] = (grad_one_eg[\"dh\" + str(i-1)])*derv\n",
        "\n",
        "    return grad_one_eg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzUNvzoD6F1s",
        "colab_type": "text"
      },
      "source": [
        "# Generating momenta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvE49Ki65_HY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createmomenta(num_hidden, sizes, inputsize = 784, outputsize = 10):\n",
        "    sizes = [inputsize] + sizes\n",
        "    sizes = sizes + [outputsize]\n",
        "    momenta = {}\n",
        "    for i in range(1, num_hidden+2):\n",
        "        momenta[\"vW\" + str(i)] = np.zeros((sizes[i], sizes[i-1]))\n",
        "        momenta[\"vb\" + str(i)] = np.zeros((sizes[i],1))\n",
        "    return momenta\n",
        "\n",
        "def createmomenta_squared(num_hidden, sizes, inputsize = 784, outputsize = 10):\n",
        "    sizes = [inputsize] + sizes\n",
        "    sizes = sizes + [outputsize]\n",
        "    momenta = {}\n",
        "    for i in range(1, num_hidden+2):\n",
        "        momenta[\"mW\" + str(i)] = np.zeros((sizes[i], sizes[i-1]))\n",
        "        momenta[\"mb\" + str(i)] = np.zeros((sizes[i],1))\n",
        "    return momenta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyQJtSzb6NBn",
        "colab_type": "text"
      },
      "source": [
        "# Functions for finding the accuracy and reading the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_pDQfdq6Hqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_accuracy(yhat,y):\n",
        "    a = np.argmax(yhat, axis = 0)\n",
        "    b = np.argmax(y, axis = 0)\n",
        "    return 100*(np.sum(a == b)/len(a))\n",
        "\n",
        "def read_data_test(path_to_csv):\n",
        "    df = pd.read_csv(path_to_csv)\n",
        "    data = df.to_numpy()\n",
        "    X = data[:,1:]\n",
        "    indices = data[:,0]\n",
        "    return data, X.T, indices\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7bB3VfY6aU7",
        "colab_type": "text"
      },
      "source": [
        "# Measuring the performance and displaying output information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV0-1idX6Qsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def measure_performance(X, Y, X_val, Y_val, params, activation_func, num_hidden, loss):\n",
        "    # Performing forward pass on training set\n",
        "    Yhat, _, _ = forward_pass(X, params, activation_func, num_hidden)\n",
        "    train_acc = find_accuracy(Yhat, Y)\n",
        "    train_err = 100 - train_acc\n",
        "    if loss == \"ce\":\n",
        "        train_loss = (cross_entropy_loss(Yhat,Y))\n",
        "    elif loss == \"sq\":\n",
        "        train_loss = (squared_loss(Yhat,Y))\n",
        "    \n",
        "    # Performing forward pass on validation set\n",
        "    Yhat_val, _, _ = forward_pass(X_val, params, activation_func, num_hidden)\n",
        "    val_acc = find_accuracy(Yhat_val, Y_val)\n",
        "    val_err = 100 - val_acc\n",
        "    if loss == \"ce\":\n",
        "        valid_loss = cross_entropy_loss(Yhat_val, Y_val)\n",
        "    elif loss == \"sq\":\n",
        "        valid_loss = (squared_loss(Yhat_val, Y_val))\n",
        "\n",
        "    return train_err, train_loss, val_err, valid_loss\n",
        "\n",
        "def display_info(epoch, train_err, train_loss, val_err, valid_loss):\n",
        "    print(\"epoch:\" + str(epoch))       \n",
        "    print(\"train error: \", \"%.2f\" % train_err, \" train loss: \", \"%.2f\" % train_loss, \n",
        "        \" validation error: \", \"%.2f\" % val_err, \"valid loss: \", \"%.2f\" % valid_loss)\n",
        "    print(\"\\n\")    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmpQfNFJ7LDp",
        "colab_type": "text"
      },
      "source": [
        "# Functions for creating submissions and log files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuOgBjoF6eyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_submission(X_test, indices, params, activation_func, num_hidden, submission_path):\n",
        "    # Prediction on the test data set\n",
        "    Yhat_test, _, _ = forward_pass(X_test, params, activation_func, num_hidden)\n",
        "    Yhat_test_classes = np.argmax(Yhat_test, axis = 0)\n",
        "    output = np.array([indices, Yhat_test_classes])\n",
        "    output = output.T\n",
        "    sub = pd.DataFrame({\"id\": output[:,0], \"label\": output[:,1]})\n",
        "    _ = sub.to_csv(submission_path, index = False)\n",
        "    print(\"Created submission at \" + submission_path)\n",
        "\n",
        "\n",
        "def create_log_files(path_expt_dir, step_data):\n",
        "    try:\n",
        "        os.mkdir(path_expt_dir)\n",
        "        print(\"expt_dir created at \" + path_expt_dir)\n",
        "    except FileExistsError:\n",
        "        print(\"expt_dir already exists at \" + path_expt_dir)\n",
        "\n",
        "    f = open(path_expt_dir + str(\"log_train.txt\"), \"w\")\n",
        "    for step in step_data:\n",
        "        line = \"Epoch \" + str(step[0]) + \", Step \" + str(step[1]) \n",
        "        line = line +  \", Loss: \" + str(np.round(step_data[step][0], decimals = 2)) # Training loss\n",
        "        line = line + \", Error: \" + str(np.round(step_data[step][1], decimals = 2)) # Training error\n",
        "        line = line + \", lr: \" + str(step_data[step][4])\n",
        "        line = line + \"\\n\" \n",
        "        f.write(line)\n",
        "    f.close()\n",
        "    \n",
        "    f = open(path_expt_dir + str(\"log_val.txt\"), \"w\")\n",
        "    for step in step_data:\n",
        "        line = \"Epoch \" + str(step[0]) + \", Step \" + str(step[1]) \n",
        "        line = line +  \", Loss: \" + str(np.round(step_data[step][2], decimals = 2)) # Validation loss\n",
        "        line = line + \", Error:\" + str(np.round(step_data[step][3], decimals = 2))  # Validation error\n",
        "        line = line + \", lr: \" + str(step_data[step][4]) \n",
        "        line = line + \"\\n\"\n",
        "        f.write(line)\n",
        "    f.close()\n",
        "    print(\"log files created\")\n",
        "\n",
        "\n",
        "# Function to create a file with all the hyper parameters used for modelling.\n",
        "def create_readme(path_expt_dir, run_details):\n",
        "    try:\n",
        "        os.mkdir(path_expt_dir)\n",
        "        print(\"expt_dir created at \" + path_expt_dir)\n",
        "    except FileExistsError:\n",
        "        print(\"expt_dir already exists at \" + path_expt_dir)\n",
        "    f = open(path_expt_dir + str(\"readme.txt\"), \"w\")\n",
        "    f.write(run_details + \"\\n\")\n",
        "    f.write\n",
        "    f.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBy1s5Oy7Xpw",
        "colab_type": "text"
      },
      "source": [
        "# Function for Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QAT--Lf7ON-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def adam(X, Y, X_val, Y_val, activation_func, loss_func, eta, num_epochs, num_hidden, sizes, batch_size, path_save_dir,\n",
        " inputsize = 784, outputsize = 10, beta1 = 0.9, beta2 = 0.999, eps = 1e-8, anneal = True, pretrain = False, state = 0):\n",
        "    print(\"Adam optimizer is being used.\")\n",
        "    step_data = {}\n",
        "    epoch_data = []\n",
        "\n",
        "    if pretrain == False:\n",
        "        params = createnetwork(num_hidden, activation_func, sizes, inputsize, outputsize)\n",
        "    elif pretrain == True:\n",
        "        params = load_params(path_save_dir, state)\n",
        "\n",
        "    prev_momenta = createmomenta(num_hidden, sizes, inputsize, outputsize)\n",
        "    momenta = createmomenta(num_hidden, sizes, inputsize, outputsize)\n",
        "    momenta_hat = createmomenta(num_hidden, sizes, inputsize, outputsize)\n",
        "\n",
        "    prev_momenta_squared = createmomenta_squared(num_hidden, sizes, inputsize, outputsize)\n",
        "    momenta_squared = createmomenta_squared(num_hidden, sizes, inputsize, outputsize)\n",
        "    momenta_squared_hat = createmomenta_squared(num_hidden, sizes, inputsize, outputsize)\n",
        "    pointsseen = 0\n",
        "    epoch = 0\n",
        "    while epoch < (num_epochs):\n",
        "        grads = creategrads(num_hidden, sizes, inputsize, outputsize)\n",
        "        step = 0\n",
        "        for j in range(0, 55000):\n",
        "            x = X[:,j]\n",
        "            y = Y[:,j]\n",
        "            yhat, A, H = forward_pass(x, params, activation_func, num_hidden)\n",
        "            grad_current = back_prop(H, A, params, num_hidden, sizes, y, yhat, loss_func, activation_func, inputsize, outputsize)\n",
        "            for key in grads:\n",
        "                grads[key] = grads[key] + grad_current[key]\n",
        "            pointsseen = pointsseen + 1\n",
        "\n",
        "            if pointsseen% batch_size == 0:\n",
        "                step = step + 1\n",
        "\n",
        "                for newkey in params:\n",
        "                  #  My stuff\n",
        "                    momenta[\"v\" + newkey] = beta2*prev_momenta[\"v\" + newkey] + (1 - beta2)*((grads[\"d\" + newkey])**2)\n",
        "                    momenta_squared[\"m\" + newkey] = beta1*prev_momenta_squared[\"m\" + newkey] + (1 - beta1)*((grads[\"d\" + newkey])**1)\n",
        "\n",
        "                    momenta_hat[\"v\" + newkey] = momenta[\"v\" + newkey]/(1 - np.power(beta2, step)) \n",
        "                    momenta_squared_hat[\"m\" + newkey] = momenta_squared[\"m\" + newkey]/(1 - np.power(beta1, step))\n",
        "\n",
        "                    params[newkey] = params[newkey] - (eta/np.sqrt(momenta_hat[\"v\" + newkey] + eps))*momenta_squared_hat[\"m\" + newkey]\n",
        "\n",
        "                    prev_momenta[\"v\" + newkey] = momenta[\"v\" + newkey]\n",
        "                    prev_momenta_squared[\"m\" + newkey] = momenta_squared[\"m\" + newkey]\n",
        "\n",
        "\n",
        "                  # His stuff\n",
        "                    # momenta[\"v\" + newkey] = beta1*prev_momenta[\"v\" + newkey] + (1 - beta1)*grads[\"d\" + newkey]\n",
        "                    # momenta_squared[\"m\" + newkey] = beta2*prev_momenta_squared[\"m\" + newkey] + (1 - beta2)*((grads[\"d\" + newkey])**2)\n",
        "\n",
        "                    # momenta_hat[\"v\" + newkey] = momenta[\"v\" + newkey]/(1 - np.power(beta1, step)) \n",
        "                    # momenta_squared_hat[\"m\" + newkey] = momenta_squared[\"m\" + newkey]/(1 - np.power(beta2, step))\n",
        "\n",
        "                    # params[newkey] = params[newkey] - (eta/np.sqrt(momenta_squared_hat[\"m\" + newkey] + eps))*momenta_hat[\"v\" + newkey]\n",
        "\n",
        "                    # prev_momenta[\"v\" + newkey] = momenta[\"v\" + newkey]\n",
        "                    # prev_momenta_squared[\"m\" + newkey] = momenta_squared[\"m\" + newkey]\n",
        "                     \n",
        "                grads = creategrads(num_hidden, sizes, inputsize, outputsize)\n",
        "\n",
        "                if step%100 == 0:\n",
        "                    train_err, train_loss, val_err, valid_loss = measure_performance(X, Y, X_val, Y_val, params, activation_func, num_hidden, loss_func)\n",
        "                    step_data[(epoch, step)] = [train_loss, train_err, valid_loss, val_err, eta]\n",
        "\n",
        "\n",
        "        train_err, train_loss, val_err, valid_loss = measure_performance(X, Y, X_val, Y_val, params, activation_func, num_hidden, loss_func)\n",
        "\n",
        "        if anneal and epoch >=1 and epoch_data[epoch - 1][2] <= valid_loss:\n",
        "            eta = eta/2\n",
        "            params = load_params(path_save_dir, epoch - 1)\n",
        "            epoch = epoch - 1\n",
        "            print(\"anneal\")\n",
        "        else: \n",
        "            display_info(epoch, train_err, train_loss, val_err, valid_loss)\n",
        "            epoch_data.append([epoch, train_loss, valid_loss])\n",
        "            pickle_params(params, epoch, path_save_dir)\n",
        "\n",
        "        epoch = epoch + 1\n",
        "    return params, step_data, epoch_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSVbUX637ey4",
        "colab_type": "text"
      },
      "source": [
        "# Function for Momentum GD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0EZlJe67ZaJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mgd(X, Y, X_val, Y_val, activation_func, loss_func, eta, gamma, num_epochs, num_hidden, sizes, batch_size, path_save_dir,\n",
        " inputsize = 784, outputsize = 10, anneal = True, pretrain = False, state = 0):\n",
        "    print(\"momentum gradient descent\")\n",
        "    step_data = {}\n",
        "    epoch_data = []\n",
        "\n",
        "    if pretrain == False:\n",
        "        params = createnetwork(num_hidden, activation_func, sizes, inputsize, outputsize)\n",
        "    elif pretrain == True:\n",
        "        params = load_params(path_save_dir, state)\n",
        "\n",
        "    prev_momenta = createmomenta(num_hidden, sizes, inputsize, outputsize)\n",
        "    momenta = createmomenta(num_hidden, sizes, inputsize, outputsize)\n",
        "    pointsseen = 0\n",
        "\n",
        "    epoch = 0\n",
        "    while epoch < num_epochs:\n",
        "        grads = creategrads(num_hidden, sizes, inputsize, outputsize)\n",
        "        step = 0\n",
        "        for j in range(0, 55000):\n",
        "            x = X[:,j]\n",
        "            y = Y[:,j]\n",
        "            yhat, A, H = forward_pass(x, params, activation_func, num_hidden)\n",
        "            grad_current = back_prop(H, A, params, num_hidden, sizes, y, yhat, loss_func, activation_func, inputsize, outputsize)\n",
        "            for key in grads:\n",
        "                grads[key] = grads[key] + grad_current[key]\n",
        "            pointsseen = pointsseen + 1\n",
        "\n",
        "            if pointsseen% batch_size == 0:\n",
        "                for newkey in params:\n",
        "                    momenta[\"v\" + newkey] = gamma*prev_momenta[\"v\" + newkey] + eta*grads[\"d\" + newkey]\n",
        "                    params[newkey] = params[newkey] - momenta[\"v\" + newkey]\n",
        "                    prev_momenta[\"v\" + newkey] = momenta[\"v\" + newkey] \n",
        "                grads = creategrads(num_hidden, sizes, inputsize, outputsize)\n",
        "\n",
        "                step = step + 1\n",
        "                if step%100 == 0:\n",
        "                    train_err, train_loss, val_err, valid_loss = measure_performance(X, Y, X_val, Y_val, params, activation_func, num_hidden, loss_func)\n",
        "                    step_data[(epoch, step)] = [train_loss, train_err, valid_loss, val_err, eta]\n",
        "       \n",
        "        train_err, train_loss, val_err, valid_loss = measure_performance(X, Y, X_val, Y_val, params, activation_func, num_hidden, loss_func)\n",
        "        \n",
        "        if anneal and epoch >=1 and epoch_data[epoch - 1][2] <= valid_loss:\n",
        "            eta = eta/2\n",
        "            params = load_params(path_save_dir, epoch - 1)\n",
        "            epoch = epoch - 1\n",
        "        else: \n",
        "            display_info(epoch, train_err, train_loss, val_err, valid_loss)\n",
        "            epoch_data.append([epoch, train_loss, valid_loss])\n",
        "            pickle_params(params, epoch, path_save_dir)\n",
        "\n",
        "        epoch = epoch + 1\n",
        "\n",
        "    return params, step_data, epoch_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdwaP6TS7l1u",
        "colab_type": "text"
      },
      "source": [
        "# Function for SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSY23z6U7h4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sgd(X, Y, X_val, Y_val, activation_func, loss_func, eta, num_epochs, num_hidden, sizes, batch_size, path_save_dir,\n",
        " inputsize = 784, outputsize = 10, anneal = True, pretrain = False, state = 0):\n",
        "    print(\"Gradient decent with minibatch is used.\")\n",
        "    step_data = {}\n",
        "    epoch_data = []\n",
        "    \n",
        "    if pretrain == False:\n",
        "        params = createnetwork(num_hidden, activation_func, sizes, inputsize, outputsize)\n",
        "    elif pretrain == True:\n",
        "        params = load_params(path_save_dir, state)\n",
        "\n",
        "\n",
        "    pointsseen = 0\n",
        "    epoch = 0\n",
        "    while epoch < num_epochs:\n",
        "        # creation of zero grad vectors at the start of every epoch\n",
        "        grads = creategrads(num_hidden, sizes, inputsize, outputsize)\n",
        "        step = 0\n",
        "\n",
        "        # iterate through every data point\n",
        "        for j in range(0, 55000):\n",
        "            x = X[:,j]\n",
        "            y = Y[:,j]\n",
        "            \n",
        "            # perform forward pass and getting a prediction \n",
        "            yhat, A, H = forward_pass(x, params, activation_func, num_hidden)\n",
        "            # performing back propagation and generating new gradients\n",
        "            grad_current = back_prop(H, A, params, num_hidden, sizes, y, yhat, loss_func, activation_func, inputsize, outputsize)\n",
        "            # cumulating the gradient values\n",
        "            for key in grads:\n",
        "                grads[key] = grads[key] + grad_current[key]\n",
        "\n",
        "            pointsseen = pointsseen + 1\n",
        "\n",
        "            # Check if a batch is complete. If so, then perform GD, update the parameters and initialize new gradients\n",
        "            if pointsseen % batch_size == 0:\n",
        "                for newkey in params:\n",
        "                    params[newkey] = params[newkey] - eta*(grads[\"d\" + newkey]/batch_size) \n",
        "                grads = creategrads(num_hidden, sizes, inputsize, outputsize)\n",
        "\n",
        "                # entering the if implies one step(batch) is done, so update step\n",
        "                step = step + 1\n",
        "                # store data for log files if 100 steps are done\n",
        "                if step%100 == 0:\n",
        "                    train_err, train_loss, val_err, valid_loss = measure_performance(X, Y, X_val, Y_val, params, activation_func, num_hidden, loss_func)\n",
        "                    step_data[(epoch, step)] = [train_loss, train_err, valid_loss, val_err, eta]\n",
        "\n",
        "        train_err, train_loss, val_err, valid_loss = measure_performance(X, Y, X_val, Y_val, params, activation_func, num_hidden, loss_func)\n",
        "\n",
        "        # Start anneling learning rate if the validation loss of the previous epoch is less than the current epoch\n",
        "        if anneal and epoch >=1 and epoch_data[epoch - 1][2] <= valid_loss:\n",
        "            eta = eta/2\n",
        "            params = load_params(path_save_dir, epoch - 1)\n",
        "            epoch = epoch - 1\n",
        "        else: \n",
        "            display_info(epoch, train_err, train_loss, val_err, valid_loss)\n",
        "            epoch_data.append([epoch, train_loss, valid_loss])\n",
        "            pickle_params(params, epoch, path_save_dir)\n",
        "\n",
        "        epoch = epoch + 1\n",
        "    return params, step_data, epoch_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z-M8-4L7tHn",
        "colab_type": "text"
      },
      "source": [
        "# Function for NAG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ5VB90U7nas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nag(X, Y, X_val, Y_val, activation_func, loss_func, eta, gamma, num_epochs, num_hidden, sizes, batch_size, path_save_dir,\n",
        " inputsize = 784, outputsize = 10, anneal = True, pretrain = False, state = 0):\n",
        "    print(\"NAG\")\n",
        "    step_data = {}\n",
        "    epoch_data = []\n",
        "\n",
        "    if pretrain == False:\n",
        "        params = createnetwork(num_hidden, activation_func, sizes, inputsize, outputsize)\n",
        "    elif pretrain == True:\n",
        "        params = load_params(path_save_dir, state)\n",
        "\n",
        "    prev_momenta = createmomenta(num_hidden, sizes, inputsize, outputsize)\n",
        "    momenta = createmomenta(num_hidden, sizes, inputsize, outputsize)\n",
        "\n",
        "    pointsseen = 0\n",
        "    epoch = 0\n",
        "    while epoch < (num_epochs):\n",
        "        grads = creategrads(num_hidden, sizes, inputsize, outputsize)\n",
        "        step = 0\n",
        "        for j in range(0, 55000):\n",
        "            x = X[:,j]\n",
        "            y = Y[:,j]\n",
        "            yhat, A, H = forward_pass(x, params, activation_func, num_hidden)\n",
        "            grad_current = back_prop(H, A, params, num_hidden, sizes, y, yhat, loss_func, activation_func, inputsize, outputsize)\n",
        "            for key in grads:\n",
        "                grads[key] = grads[key] + grad_current[key]\n",
        "            pointsseen = pointsseen + 1\n",
        "            if pointsseen% batch_size == 0:\n",
        "                step = step + 1\n",
        "\n",
        "                for newkey in params:\n",
        "                    momenta[\"v\" + newkey] = gamma*prev_momenta[\"v\" + newkey] + eta*grads[\"d\" + newkey]\n",
        "                    params[newkey] = params[newkey] - momenta[\"v\" + newkey]\n",
        "                    prev_momenta[\"v\" + newkey] = momenta[\"v\" + newkey] \n",
        "                grads = creategrads(num_hidden, sizes, inputsize, outputsize)\n",
        "\n",
        "                if step%100 == 0:\n",
        "                    train_err, train_loss, val_err, valid_loss = measure_performance(X, Y, X_val, Y_val, params, activation_func, num_hidden, loss_func)\n",
        "                    step_data[(epoch, step)] = [train_loss, train_err, valid_loss, val_err, eta]\n",
        "\n",
        "                for next_key in params:\n",
        "                    # Try to place this in the above for loop------------------------------------------------------------------------------------------------------------------\n",
        "                    momenta[\"v\" + next_key] = gamma*prev_momenta[\"v\" + next_key]\n",
        "                    params[next_key] = params[next_key] - momenta[\"v\" + next_key]\n",
        "\n",
        "        train_err, train_loss, val_err, valid_loss = measure_performance(X, Y, X_val, Y_val, params, activation_func, num_hidden, loss_func)\n",
        "\n",
        "        if anneal and epoch >=1 and epoch_data[epoch - 1][2] <= valid_loss:\n",
        "            eta = eta/2\n",
        "            params = load_params(path_save_dir, epoch - 1)\n",
        "            epoch = epoch - 1\n",
        "            print(\"anneal\")\n",
        "        else: \n",
        "            display_info(epoch, train_err, train_loss, val_err, valid_loss)\n",
        "            epoch_data.append([epoch, train_loss, valid_loss])\n",
        "            pickle_params(params, epoch, path_save_dir)\n",
        "\n",
        "\n",
        "        epoch = epoch + 1\n",
        "    return params, step_data, epoch_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8R02uvJ8EO4",
        "colab_type": "text"
      },
      "source": [
        "# Function for reading train validation and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nWEL2uD7uhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read train, validation and test data\n",
        "def init_data(path_train, path_val, path_test):\n",
        "    data, X, y = read_data(path_train)\n",
        "    \n",
        "    Y = (convert_to_onehot(y, 10)).T\n",
        "    X = (X.T/255)\n",
        "    pca = PCA(n_components=50)\n",
        "    pca.fit(X)\n",
        "    X = pca.transform(X)\n",
        "    X = X.T\n",
        "    print(np.shape(X), np.shape(y), np.shape(data))\n",
        "\n",
        "    data, X_val, y_val = read_data(path_val)\n",
        "    \n",
        "    Y_val = (convert_to_onehot(y_val, 10)).T\n",
        "    X_val = (X_val.T/255)\n",
        "    X_val = pca.transform(X_val)\n",
        "    X_val = X_val.T\n",
        "    print(np.shape(X_val), np.shape(y_val), np.shape(data))\n",
        "\n",
        "\n",
        "    data, X_test, indices = read_data_test(path_test)\n",
        "\n",
        "    X_test = X_test.T/255\n",
        "    X_test = pca.transform(X_test)\n",
        "    X_test = X_test.T\n",
        "    #print(np.shape(X_test), \"shape of test data\")\n",
        "    #np.savetxt(\"test_2.csv\",X_test, delimiter = \",\")\n",
        "\n",
        "    return X, Y, X_val, Y_val, X_test, indices\n",
        "\n",
        "\n",
        "def pickle_params(params, epoch, path_save_dir):\n",
        "    try:\n",
        "        os.mkdir(path_save_dir)\n",
        "        print(\"save_dir created at \" + path_save_dir)\n",
        "    except FileExistsError:\n",
        "        print(\"save_dir already exists at \" + path_save_dir)\n",
        "    filename = path_save_dir + \"weights_\" + str(epoch) + \".pickle\"    \n",
        "    with open(filename, 'wb') as handle:\n",
        "        pickle.dump(params, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_params(path_save_dir, epoch):\n",
        "    if os.path.isdir(path_save_dir):\n",
        "        filename = path_save_dir + \"weights_\" + str(epoch) + \".pickle\"\n",
        "        with open(filename, 'rb') as handle:\n",
        "            parameters = pickle.load(handle)\n",
        "        return parameters \n",
        "    else:\n",
        "        print(\"No directory at \" + path_save_dir)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIL_jCwj8TSD",
        "colab_type": "text"
      },
      "source": [
        "# Using all the above functions wih different test cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjxGF_NR8HG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "# setting variables globally \n",
        "train = Path('/content/train.csv')\n",
        "val = Path('/content/valid.csv')\n",
        "test = Path('/content/test.csv')\n",
        "save_dir= '/content/save_dir'\n",
        "expt_dir= '/content/expt_dir'\n",
        "\n",
        "path_save_dir = save_dir\n",
        "path_expt_dir = expt_dir\n",
        "path_train = train\n",
        "path_val = val\n",
        "path_test = test\n",
        "pretrain = False\n",
        "testing = False "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILz_Wz2hiRn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(eta, gamma, num_hidden, sizes, activation_func, loss_func, optim, batch_size, num_epochs, path_save_dir, path_expt_dir, path_train, path_val, path_test, anneal, state):\n",
        "  # Reading data\n",
        "  if testing == False:\n",
        "      X, Y, X_val, Y_val, X_test, indices = init_data(path_train, path_val, path_test)\n",
        "  elif testing == True:\n",
        "      data = pd.read_csv(path_test, header = None)\n",
        "      X_test = data.to_numpy()\n",
        "      indices = np.arange(np.shape(data)[1])\n",
        "\n",
        "  # Training part\n",
        "  if testing == False:\n",
        "      if optim == \"gd\":\n",
        "          params, step_data, epoch_data = sgd(X, Y, X_val, Y_val, activation_func, loss_func, eta, num_epochs, num_hidden, sizes, batch_size, path_save_dir, inputsize = np.shape(X)[0], anneal = anneal, pretrain = pretrain, state = state)\n",
        "      elif optim == \"momentum\":\n",
        "          params, step_data, epoch_data = mgd(X, Y, X_val, Y_val, activation_func, loss_func, eta, gamma, num_epochs, num_hidden, sizes, batch_size, path_save_dir, inputsize = np.shape(X)[0], anneal = anneal, pretrain = pretrain, state = state)\n",
        "      elif optim == \"adam\":\n",
        "          params, step_data, epoch_data = adam(X, Y, X_val, Y_val, activation_func, loss_func, eta, num_epochs, num_hidden, sizes, batch_size, path_save_dir, inputsize = np.shape(X)[0], anneal = anneal, pretrain = pretrain, state = state)\n",
        "      elif optim == \"nag\":\n",
        "          params, step_data, epoch_data = nag(X, Y, X_val, Y_val, activation_func, loss_func, eta, gamma, num_epochs, num_hidden, sizes, batch_size, path_save_dir, inputsize = np.shape(X)[0], anneal = anneal, pretrain = pretrain, state = state)\n",
        "     \n",
        "      # Testing part\n",
        "      create_log_files(path_expt_dir, step_data)\n",
        "      # create_readme(path_expt_dir, run_details)\n",
        "      submission_path = path_expt_dir + \"test_submission.csv\"\n",
        "  else:\n",
        "      params = load_params(path_save_dir, state)\n",
        "      num_hidden = int(len(params.keys())/2 - 1)\n",
        "      print(\"num_hidden\", num_hidden)\n",
        "      activation_func = \"relu\"\n",
        "      submission_path = path_expt_dir + \"predictions_\" + str(state) + \".csv\"\n",
        "\n",
        "\n",
        "  create_submission(X_test, indices, params, activation_func, num_hidden, submission_path)\n",
        "  return epoch_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bMLr-UXSa73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_stuff(result_list, color_list, description_list, title):\n",
        "  plt.figure(figsize=(10,8))\n",
        "  for i in range(0,len(result_list)): \n",
        "    color = color_list[i]\n",
        "    des = description_list[i]\n",
        "    epoch_data = result_list[i]\n",
        "    epoch_data_np = np.asarray(epoch_data)\n",
        "    plt.plot(epoch_data_np.T[0], epoch_data_np.T[1], color,label=des)  \n",
        "  \n",
        "  plt.xlabel(\"Number of Iterations\")\n",
        "  plt.ylabel(\"Loss values\")\n",
        "  plt.grid()\n",
        "  plt.title(title[0])\n",
        "  plt.legend()\n",
        "  plt.show()  \n",
        "\n",
        "  plt.figure(figsize=(10,8))\n",
        "  for i in range(0,len(result_list)):\n",
        "    color = color_list[i]\n",
        "    des = description_list[i]\n",
        "    epoch_data = result_list[i]\n",
        "    epoch_data_np = np.asarray(epoch_data)\n",
        "    plt.plot(epoch_data_np.T[0], epoch_data_np.T[2], color,label=des)\n",
        "\n",
        "  plt.xlabel(\"Number of Iterations\")\n",
        "  plt.ylabel(\"Loss values\")\n",
        "  plt.grid()\n",
        "  plt.title(title[1])\n",
        "  plt.legend()\n",
        "  plt.show()  \n",
        "\n",
        "  return "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVUFLyP6C1Ar",
        "colab_type": "text"
      },
      "source": [
        "For all the above 4 cases you will use sigmoid activation, cross entropy loss, Adam,\n",
        "batch size 20 and tune the learning rate to get best results. For each of the 4 questions\n",
        "above you need to draw the following plots:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M55qhaPzlhfl",
        "colab_type": "text"
      },
      "source": [
        "# Training and Validation loss for different sizes with two hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vyPnXsk_0K4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_50 = run_model(0.005, 0.5, 2, [50,50], 'relu', 'ce', 'adam', 20, 10, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "result_100 = run_model(0.005, 0.5, 2, [100,100], 'relu', 'ce', 'adam', 20, 10, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "result_200 = run_model(0.005, 0.5, 2, [200,200], 'relu', 'ce', 'adam', 20, 10, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "result_300 = run_model(0.005, 0.5, 2, [300,300], 'relu', 'ce', 'adam', 20, 10, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "\n",
        "result_list = [result_50, result_100, result_200, result_300]\n",
        "color_list = ['r', 'b', 'g', 'm']\n",
        "description_list = ['50 hidden units', '100 hidden units', '200 hidden units', '300 hidden units']\n",
        "title = ['Training loss', 'Validation loss']\n",
        "\n",
        "plot_stuff(result_list, color_list, description_list, title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzHNMH6Fm60J",
        "colab_type": "text"
      },
      "source": [
        "# Training and Validation loss for different sizes with one hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x_7y8qwBq9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # (eta, gamma, num_hidden, sizes, activation_func, loss_func, optim, batch_size, num_epochs, path_save_dir, path_expt_dir, path_train, path_val, path_test, anneal, state)\n",
        "result_50_1 = run_model(0.005, 0.5, 1, [50], 'relu', 'ce', 'adam', 20, 10, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "result_100_1 = run_model(0.005, 0.5, 1, [100], 'relu', 'ce', 'adam', 20, 10, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "result_200_1 = run_model(0.005, 0.5, 1, [200], 'relu', 'ce', 'adam', 20, 10, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "result_300_1 = run_model(0.005, 0.5, 1, [300], 'relu', 'ce', 'adam', 20, 10, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "\n",
        "result_list = [result_50_1, result_100_1, result_200_1, result_300_1]\n",
        "color_list = ['r', 'b', 'g', 'm']\n",
        "description_list = ['50 hidden units', '100 hidden units', '200 hidden units', '300 hidden units']\n",
        "title = ['Training loss', 'Validation loss']\n",
        "\n",
        "plot_stuff(result_list, color_list, description_list, title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q9benamm6u-",
        "colab_type": "text"
      },
      "source": [
        "# Training and Validation loss for different sizes with three hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcJfacsdBxiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # (eta, gamma, num_hidden, sizes, activation_func, loss_func, optim, batch_size, num_epochs, path_save_dir, path_expt_dir, path_train, path_val, path_test, anneal, state)\n",
        "result_50_3 = run_model(0.005, 0.5, 3, [50, 50, 50], 'relu', 'ce', 'adam', 20, 15, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "result_100_3 = run_model(0.005, 0.5, 3, [75, 75, 75], 'relu', 'ce', 'adam', 20, 15, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "result_200_3 = run_model(0.005, 0.5, 3, [100, 100, 100], 'relu', 'ce', 'adam', 20, 15, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "result_300_3 = run_model(0.005, 0.5, 3, [125, 125, 125], 'relu', 'ce', 'adam', 20, 15, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "\n",
        "result_list = [result_50_3, result_100_3, result_200_3, result_300_3]\n",
        "color_list = ['r', 'b', 'g', 'm']\n",
        "description_list = ['50 hidden units', '100 hidden units', '200 hidden units', '300 hidden units']\n",
        "title = ['Training loss', 'Validation loss']\n",
        "\n",
        "plot_stuff(result_list, color_list, description_list, title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzGHLoC8m6qY",
        "colab_type": "text"
      },
      "source": [
        "# Training and Validation loss for different sizes with four hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXhGCacwByn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # (eta, gamma, num_hidden, sizes, activation_func, loss_func, optim, batch_size, num_epochs, path_save_dir, path_expt_dir, path_train, path_val, path_test, anneal, state)\n",
        "result_50_4 = run_model(0.005, 0.5, 4, [50, 50, 50, 50], 'relu', 'ce', 'adam', 20, 15, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "result_100_4 = run_model(0.005, 0.5, 4, [75, 75, 75, 75], 'relu', 'ce', 'adam', 20, 15, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "result_200_4 = run_model(0.005, 0.5, 4, [100, 100, 100, 100], 'relu', 'ce', 'adam', 20, 15, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "result_300_4 = run_model(0.005, 0.5, 4, [125, 125, 125, 125], 'relu', 'ce', 'adam', 20, 15, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "\n",
        "result_list = [result_50_4, result_100_4, result_200_4, result_300_4]\n",
        "color_list = ['r', 'b', 'g', 'm']\n",
        "description_list = ['50 hidden units', '75 hidden units', '100 hidden units', '125 hidden units']\n",
        "title = ['Training loss', 'Validation loss']\n",
        "\n",
        "plot_stuff(result_list, color_list, description_list, title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBgKiqJ1m6kV",
        "colab_type": "text"
      },
      "source": [
        "# Training and Validation loss for different learning algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H06CKhakB2Zm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # (eta, gamma, num_hidden, sizes, activation_func, loss_func, optim, batch_size, num_epochs, path_save_dir, path_expt_dir, path_train, path_val, path_test, anneal, state)\n",
        "result_adam = run_model(0.005, 0.5, 2, [50, 50], 'relu', 'ce', 'adam', 20, 15, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "result_nag = run_model(0.005, 0.5, 2, [50, 50], 'relu', 'ce', 'nag', 20, 15, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "result_mgd = run_model(0.005, 0.5, 2, [50, 50], 'relu', 'ce', 'momentum', 20, 15, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWef-bQGI53J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_list = [result_adam, result_nag, result_mgd]\n",
        "color_list = ['r', 'b', 'g']\n",
        "description_list = ['Adam', 'Nestrov accelerated GD', 'Momentum GD']\n",
        "title = ['Training loss', 'Validation loss']\n",
        "\n",
        "plot_stuff(result_list, color_list, description_list, title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzwlo7gcm6hF",
        "colab_type": "text"
      },
      "source": [
        "# Training and Validation loss for different activation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tusMg8iZB5TV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_sig = run_model(0.005, 0.5, 2, [100, 100], 'sigmoid', 'ce', 'adam', 20, 15, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "result_tanh = run_model(0.005, 0.5, 2, [100, 100], 'tanh', 'ce', 'adam', 20, 15, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "\n",
        "result_list = [result_sig, result_tanh]\n",
        "color_list = ['r', 'b']\n",
        "description_list = ['Sigmoid activation', 'Tanh activation']\n",
        "title = ['Training loss', 'Validation loss']\n",
        "\n",
        "plot_stuff(result_list, color_list, description_list, title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgFmg1k3m6ds",
        "colab_type": "text"
      },
      "source": [
        "# Training and Validation loss for different Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqd-wdacCNK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_cross = run_model(0.005, 0.5, 2, [100, 100], 'sigmoid', 'ce', 'adam', 20, 15, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "result_square = run_model(0.005, 0.5, 2, [100, 100], 'tanh', 'sq', 'adam', 20, 15, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "\n",
        "result_list = [result_cross, result_square]\n",
        "color_list = ['r', 'b']\n",
        "description_list = ['Cross entropy loss', 'Squared error loss']\n",
        "title = ['Training loss', 'Validation loss']\n",
        "\n",
        "plot_stuff(result_list, color_list, description_list, title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N_4Q_g-nUJs",
        "colab_type": "text"
      },
      "source": [
        "# Training and Validation loss for different batch sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SJN1uRbl97G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_1 = run_model(0.005, 0.5, 2, [100, 100], 'sigmoid', 'ce', 'adam', 1, 15, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "result_20 = run_model(0.005, 0.5, 2, [100, 100], 'sigmoid', 'ce', 'adam', 20, 15, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "result_100 = run_model(0.005, 0.5, 2, [100, 100], 'sigmoid', 'ce', 'adam', 100, 15, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "result_1000 = run_model(0.005, 0.5, 2, [100, 100], 'sigmoid', 'ce', 'adam', 1000, 15, path_save_dir, path_expt_dir, path_train, path_val, path_test, True, 0)\n",
        "\n",
        "result_list = [result_1, result_20, result_100, result_1000]\n",
        "color_list = ['r', 'b', 'g', 'm']\n",
        "description_list = ['Batch size = 1', 'Batch size = 20', 'Batch size = 100', 'Batch size = 1000']\n",
        "title = ['Training loss', 'Validation loss']\n",
        "\n",
        "plot_stuff(result_list, color_list, description_list, title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyYKETfIDYnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}